{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwoTLFmKciVW"
   },
   "source": [
    "#### API models fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dAa7OCAa6eh"
   },
   "outputs": [],
   "source": [
    "# Google Drive 마운트 및 필요한 라이브러리 설치\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# !pip install anthropic tqdm seaborn pandas numpy matplotlib scikit-learn\n",
    "# !pip install --upgrade anthropic\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from anthropic import Anthropic\n",
    "import time\n",
    "\n",
    "# Google Drive 내 작업 디렉토리 설정\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/kt/edusmile\"\n",
    "DATA_PATH = f\"{DRIVE_PATH}/label_data\"\n",
    "\n",
    "# 필요한 디렉토리 생성\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "for split in ['train', 'test', 'val']:\n",
    "    os.makedirs(f\"{DATA_PATH}/{split}\", exist_ok=True)\n",
    "\n",
    "class LLMClassifier:\n",
    "    def __init__(self, api_key):\n",
    "        \"\"\"\n",
    "        LLM 분류기 초기화\n",
    "        \"\"\"\n",
    "        self.anthropic = Anthropic(api_key=api_key)\n",
    "        self.categories = ['수업내용', '공지사항', '잡담']\n",
    "        self.system_prompt = \"\"\"\n",
    "        당신은 텍스트 분류 모델입니다. 주어진 텍스트를 다음 카테고리 중 하나로 분류해야 합니다:\n",
    "        - 수업내용: 강의나 수업과 관련된 내용\n",
    "        - 공지사항: 공지나 안내사항\n",
    "        - 잡담: 일상적인 대화나 잡담\n",
    "\n",
    "        카테고리명만 정확히 응답해주세요.\n",
    "        \"\"\"\n",
    "\n",
    "    def predict_single(self, text):\n",
    "        \"\"\"\n",
    "        단일 텍스트에 대한 예측을 수행합니다.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.anthropic.messages.create(\n",
    "                model=\"claude-3-sonnet-20240229\",\n",
    "                max_tokens=10,\n",
    "                temperature=0,\n",
    "                system=self.system_prompt,\n",
    "                messages=[{\"role\": \"user\", \"content\": text}]\n",
    "            )\n",
    "            prediction = response.content[0].text.strip()\n",
    "            return prediction\n",
    "        except Exception as e:\n",
    "            print(f\"예측 중 오류 발생: {e}\")\n",
    "            return None\n",
    "\n",
    "    def predict_batch(self, texts, batch_size=5):\n",
    "        \"\"\"\n",
    "        배치 단위로 예측을 수행합니다.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_predictions = []\n",
    "            for text in batch:\n",
    "                prediction = self.predict_single(text)\n",
    "                batch_predictions.append(prediction)\n",
    "                time.sleep(1)  # API 호출 제한 고려\n",
    "            predictions.extend(batch_predictions)\n",
    "        return predictions\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred, output_path=None):\n",
    "        \"\"\"\n",
    "        혼동 행렬을 시각화합니다.\n",
    "        \"\"\"\n",
    "        # None 값 제거\n",
    "        valid_indices = [i for i, pred in enumerate(y_pred) if pred is not None]\n",
    "        y_true = [y_true[i] for i in valid_indices]\n",
    "        y_pred = [y_pred[i] for i in valid_indices]\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=self.categories)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=self.categories,\n",
    "                   yticklabels=self.categories)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "\n",
    "        if output_path:\n",
    "            plt.savefig(output_path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "def main():\n",
    "    # API 키 설정\n",
    "    api_key = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "    # 분류기 초기화\n",
    "    classifier = LLMClassifier(api_key)\n",
    "\n",
    "    # 데이터 로드\n",
    "    splits = {}\n",
    "    for split in ['val', 'test']:\n",
    "        file_path = f\"{DATA_PATH}/{split}/{split}_dataset.jsonl\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            splits[split] = [json.loads(line) for line in f]\n",
    "        print(f\"{split} 데이터 로드 완료: {len(splits[split])} 샘플\")\n",
    "\n",
    "    # Validation 평가\n",
    "    print(\"\\nValidation 데이터 평가 중...\")\n",
    "    val_texts = [item['instruction'] for item in splits['val']]\n",
    "    val_labels = [item['category'] for item in splits['val']]\n",
    "\n",
    "    val_predictions = classifier.predict_batch(val_texts)\n",
    "\n",
    "    # None 값 제거\n",
    "    val_valid_indices = [i for i, pred in enumerate(val_predictions) if pred is not None]\n",
    "    val_labels_clean = [val_labels[i] for i in val_valid_indices]\n",
    "    val_predictions_clean = [val_predictions[i] for i in val_valid_indices]\n",
    "\n",
    "    print(\"\\nValidation 결과:\")\n",
    "    print(classification_report(val_labels_clean, val_predictions_clean))\n",
    "\n",
    "    # Test 평가\n",
    "    print(\"\\nTest 데이터 평가 중...\")\n",
    "    test_texts = [item['instruction'] for item in splits['test']]\n",
    "    test_labels = [item['category'] for item in splits['test']]\n",
    "\n",
    "    test_predictions = classifier.predict_batch(test_texts)\n",
    "\n",
    "    # None 값 제거\n",
    "    test_valid_indices = [i for i, pred in enumerate(test_predictions) if pred is not None]\n",
    "    test_labels_clean = [test_labels[i] for i in test_valid_indices]\n",
    "    test_predictions_clean = [test_predictions[i] for i in test_valid_indices]\n",
    "\n",
    "    print(\"\\nTest 결과:\")\n",
    "    print(classification_report(test_labels_clean, test_predictions_clean))\n",
    "\n",
    "    # 혼동 행렬 저장\n",
    "    output_path = f\"{DATA_PATH}/confusion_matrix.png\"\n",
    "    classifier.plot_confusion_matrix(\n",
    "        test_labels,\n",
    "        test_predictions,\n",
    "        output_path=output_path\n",
    "    )\n",
    "    print(f\"\\n혼동 행렬이 저장되었습니다: {output_path}\")\n",
    "\n",
    "    # 결과 저장\n",
    "    results = {\n",
    "        'test_metrics': classification_report(test_labels_clean, test_predictions_clean, target_names=classifier.categories, output_dict=True),\n",
    "        'val_metrics': classification_report(val_labels_clean, val_predictions_clean, target_names=classifier.categories, output_dict=True),\n",
    "    }\n",
    "\n",
    "    results_path = f\"{DATA_PATH}/evaluation_results.json\"\n",
    "    with open(results_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\n평가 결과가 저장되었습니다: {results_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyzA__OfHuBt"
   },
   "outputs": [],
   "source": [
    "# gpt_training.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "\n",
    "class GPTClassifier:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.categories = ['수업내용', '공지사항', '잡담']\n",
    "\n",
    "    def list_active_jobs(self):\n",
    "        \"\"\"현재 실행 중인 파인튜닝 작업을 확인합니다.\"\"\"\n",
    "        jobs = self.client.fine_tuning.jobs.list(limit=10)\n",
    "        active_jobs = []\n",
    "        for job in jobs:\n",
    "            if job.status in ['running', 'pending']:\n",
    "                active_jobs.append(job)\n",
    "                print(f\"Active Job ID: {job.id}\")\n",
    "                print(f\"Status: {job.status}\")\n",
    "                print(f\"Model: {job.model}\")\n",
    "                print(f\"Created at: {job.created_at}\")\n",
    "                print(\"---\")\n",
    "        return active_jobs\n",
    "\n",
    "    def cancel_job(self, job_id):\n",
    "        \"\"\"파인튜닝 작업을 취소합니다.\"\"\"\n",
    "        try:\n",
    "            self.client.fine_tuning.jobs.cancel(job_id)\n",
    "            print(f\"Job {job_id} cancelled successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error cancelling job: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_existing_data(self, file_path):\n",
    "        \"\"\"기존 JSONL 파일을 로드하고 OpenAI fine-tuning 형식으로 변환합니다.\"\"\"\n",
    "        data = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a text classifier that categorizes Korean educational text into three categories: '수업내용' (class content), '공지사항' (announcements), and '잡담' (chat).\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": item['instruction']\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": item['category']\n",
    "                    }\n",
    "                ]\n",
    "                data.append({\"messages\": messages})\n",
    "        return data\n",
    "\n",
    "    def save_training_data(self, data, output_file):\n",
    "        \"\"\"파인튜닝 데이터를 JSONL 형식으로 저장합니다.\"\"\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        print(f\"Training data saved to: {output_file}\")\n",
    "        print(f\"File size: {os.path.getsize(output_file) / 1024:.2f} KB\")\n",
    "\n",
    "    def create_fine_tuning_job(self, training_file):\n",
    "        \"\"\"파인튜닝 작업을 생성합니다.\"\"\"\n",
    "        try:\n",
    "            # 먼저 활성 작업 확인\n",
    "            active_jobs = self.list_active_jobs()\n",
    "            if len(active_jobs) >= 3:\n",
    "                print(\"\\n현재 3개의 활성 작업이 실행 중입니다.\")\n",
    "                cancel = input(\"오래된 작업을 취소하시겠습니까? (y/n): \")\n",
    "                if cancel.lower() == 'y':\n",
    "                    # 가장 오래된 작업 취소\n",
    "                    oldest_job = min(active_jobs, key=lambda x: x.created_at)\n",
    "                    if self.cancel_job(oldest_job.id):\n",
    "                        time.sleep(10)  # 작업 취소 후 대기\n",
    "                    else:\n",
    "                        return None\n",
    "                else:\n",
    "                    return None\n",
    "\n",
    "            print(\"\\nUploading training file...\")\n",
    "            file = self.client.files.create(\n",
    "                file=open(training_file, 'rb'),\n",
    "                purpose=\"fine-tune\"\n",
    "            )\n",
    "            print(f\"File uploaded with ID: {file.id}\")\n",
    "\n",
    "            # 파일 상태 확인\n",
    "            while True:\n",
    "                file_status = self.client.files.retrieve(file.id)\n",
    "                if file_status.status == \"processed\":\n",
    "                    break\n",
    "                print(f\"Waiting for file processing... Status: {file_status.status}\")\n",
    "                time.sleep(5)\n",
    "\n",
    "            print(\"\\nCreating fine-tuning job...\")\n",
    "            job = self.client.fine_tuning.jobs.create(\n",
    "                training_file=file.id,\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                hyperparameters={\n",
    "                    \"n_epochs\": 3\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return job.id\n",
    "        except Exception as e:\n",
    "            print(f\"파인튜닝 작업 생성 중 오류 발생: {e}\")\n",
    "            return None\n",
    "\n",
    "    def monitor_fine_tuning_job(self, job_id):\n",
    "        \"\"\"파인튜닝 작업의 진행 상황을 모니터링합니다.\"\"\"\n",
    "        print(f\"\\nMonitoring fine-tuning job: {job_id}\")\n",
    "        previous_status = None\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                job = self.client.fine_tuning.jobs.retrieve(job_id)\n",
    "                current_status = job.status\n",
    "\n",
    "                if current_status != previous_status:\n",
    "                    print(f\"\\nStatus changed: {current_status}\")\n",
    "                    previous_status = current_status\n",
    "\n",
    "                if hasattr(job, 'trained_tokens'):\n",
    "                    print(f\"Trained tokens: {job.trained_tokens}\")\n",
    "\n",
    "                if current_status == \"succeeded\":\n",
    "                    print(f\"\\nFine-tuning completed successfully!\")\n",
    "                    print(f\"Fine-tuned model ID: {job.fine_tuned_model}\")\n",
    "                    return True, job.fine_tuned_model\n",
    "                elif current_status == \"failed\":\n",
    "                    print(f\"\\nFine-tuning failed!\")\n",
    "                    if hasattr(job, 'error'):\n",
    "                        print(f\"Error: {job.error}\")\n",
    "                    return False, None\n",
    "\n",
    "                time.sleep(30)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Monitoring error: {e}\")\n",
    "                time.sleep(30)\n",
    "\n",
    "    def predict_single(self, text, model_id=None):\n",
    "        \"\"\"단일 텍스트에 대한 예측을 수행합니다.\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model_id if model_id else \"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a text classifier that categorizes Korean educational text into three categories: '수업내용', '공지사항', '잡담'.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": text\n",
    "                    }\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=10\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"예측 중 오류 발생: {e}\")\n",
    "            return None\n",
    "\n",
    "    def predict_batch(self, texts, model_id=None, batch_size=5):\n",
    "        \"\"\"배치 단위로 예측을 수행합니다.\"\"\"\n",
    "        predictions = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_predictions = []\n",
    "            for text in batch:\n",
    "                prediction = self.predict_single(text, model_id)\n",
    "                batch_predictions.append(prediction)\n",
    "                time.sleep(1)  # API 호출 제한 고려\n",
    "            predictions.extend(batch_predictions)\n",
    "        return predictions\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred, output_path=None):\n",
    "        \"\"\"혼동 행렬을 시각화합니다.\"\"\"\n",
    "        # None 값 제거\n",
    "        valid_indices = [i for i, pred in enumerate(y_pred) if pred is not None]\n",
    "        y_true = [y_true[i] for i in valid_indices]\n",
    "        y_pred = [y_pred[i] for i in valid_indices]\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=self.categories)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=self.categories,\n",
    "                   yticklabels=self.categories)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "\n",
    "        if output_path:\n",
    "            plt.savefig(output_path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEYAuvCaKZem"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 경로 설정\n",
    "    TRAIN_PATH = \"/content/drive/MyDrive/kt/edusmile/label_data/train/train_dataset.jsonl\"\n",
    "    VAL_PATH = \"/content/drive/MyDrive/kt/edusmile/label_data/val/val_dataset.jsonl\"\n",
    "    TEST_PATH = \"/content/drive/MyDrive/kt/edusmile/label_data/test/test_dataset.jsonl\"\n",
    "    OUTPUT_PATH = \"/content/drive/MyDrive/kt/edusmile/label_data\"\n",
    "\n",
    "    # API 키 설정\n",
    "    api_key = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "    # 분류기 초기화\n",
    "    classifier = GPTClassifier(api_key)\n",
    "\n",
    "    # 기존 데이터 로드 및 변환\n",
    "    print(\"\\n기존 데이터 로드 및 변환 중...\")\n",
    "    training_data = classifier.load_existing_data(TRAIN_PATH)\n",
    "    print(f\"학습 데이터 로드 완료: {len(training_data)} 샘플\")\n",
    "\n",
    "    # OpenAI fine-tuning 형식으로 변환된 데이터 저장\n",
    "    finetune_file = os.path.join(OUTPUT_PATH, \"openai_finetune.jsonl\")\n",
    "    classifier.save_training_data(training_data, finetune_file)\n",
    "\n",
    "    # 파인튜닝 수행\n",
    "    print(\"\\n파인튜닝 시작...\")\n",
    "    job_id = classifier.create_fine_tuning_job(finetune_file)\n",
    "\n",
    "    if job_id:\n",
    "        print(f\"\\n파인튜닝 작업 ID: {job_id}\")\n",
    "        success, model_id = classifier.monitor_fine_tuning_job(job_id)\n",
    "\n",
    "        if success:\n",
    "            print(f\"\\n파인튜닝 완료! 모델 ID: {model_id}\")\n",
    "\n",
    "            # Validation 평가\n",
    "            print(\"\\nValidation 데이터 평가 중...\")\n",
    "            val_data = classifier.load_existing_data(VAL_PATH)\n",
    "            val_texts = [item['messages'][1]['content'] for item in val_data]\n",
    "            val_labels = [item['messages'][2]['content'] for item in val_data]\n",
    "\n",
    "            val_predictions = classifier.predict_batch(val_texts, model_id)\n",
    "            print(\"\\nValidation 결과:\")\n",
    "            print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "            # Test 평가\n",
    "            print(\"\\nTest 데이터 평가 중...\")\n",
    "            test_data = classifier.load_existing_data(TEST_PATH)\n",
    "            test_texts = [item['messages'][1]['content'] for item in test_data]\n",
    "            test_labels = [item['messages'][2]['content'] for item in test_data]\n",
    "\n",
    "            test_predictions = classifier.predict_batch(test_texts, model_id)\n",
    "            print(\"\\nTest 결과:\")\n",
    "            print(classification_report(test_labels, test_predictions))\n",
    "\n",
    "            # 혼동 행렬 저장\n",
    "            output_path = os.path.join(OUTPUT_PATH, \"confusion_matrix.png\")\n",
    "            classifier.plot_confusion_matrix(\n",
    "                test_labels,\n",
    "                test_predictions,\n",
    "                output_path=output_path\n",
    "            )\n",
    "            print(f\"\\n혼동 행렬이 저장되었습니다: {output_path}\")\n",
    "\n",
    "            # 결과 저장\n",
    "            results = {\n",
    "                'model_id': model_id,\n",
    "                'test_metrics': classification_report(test_labels, test_predictions, target_names=classifier.categories, output_dict=True),\n",
    "                'val_metrics': classification_report(val_labels, val_predictions, target_names=classifier.categories, output_dict=True),\n",
    "            }\n",
    "\n",
    "            results_path = os.path.join(OUTPUT_PATH, \"evaluation_results.json\")\n",
    "            with open(results_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"\\n평가 결과가 저장되었습니다: {results_path}\")\n",
    "        else:\n",
    "            print(\"\\n파인튜닝 실패\")\n",
    "    else:\n",
    "        print(\"\\n파인튜닝 작업 생성 실패\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMN81YvD/GhmDBiJZ9YYEca",
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
